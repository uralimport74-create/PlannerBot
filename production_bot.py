# production_bot.py

import os
import sys
import io
import math
import re
from datetime import datetime
import pandas as pd
import pytz
import gspread
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from oauth2client.service_account import ServiceAccountCredentials
import telebot

import config # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

# --- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ö–û–ù–°–¢–ê–ù–¢–´ ---
TIMEZONE = 'Asia/Yekaterinburg'
DAY_MAP = ['–ü–Ω', '–í—Ç', '–°—Ä', '–ß—Ç', '–ü—Ç', '–°–±', '–í—Å']

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ª–æ–≥–∏–∫–∏
COL_NAME_NOMENCLATURE = "–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞"
COL_NAME_STOCK = "–û—Å—Ç–∞—Ç–æ–∫ —É—á–∏—Ç—ã–≤–∞—è —Ä–µ–∑–µ—Ä–≤ –∫–æ—Ä."
COL_NAME_SALES = "–ü—Ä–æ–¥–∞–∂–∏ –∑–∞ 2 –Ω–µ–¥–µ–ª–∏"
COL_NAME_PACK = "–í–ª–æ–∂–µ–Ω–∏–µ"

# --- 4. –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö GOOGLE API ---

def get_creds():
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    try:
        return ServiceAccountCredentials.from_json_keyfile_name(config.CREDENTIALS_FILE, scope)
    except Exception as e:
        raise ConnectionError(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö '{config.CREDENTIALS_FILE}': {e}")

def connect_services():
    creds = get_creds()
    gc = gspread.authorize(creds)
    drive_service = build('drive', 'v3', credentials=creds)
    return gc, drive_service

# --- 5. –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –û–®–ò–ë–û–ö ---

def log_error_to_sheet(gc, error_message, file_name="System"):
    try:
        tz = pytz.timezone(TIMEZONE)
        now_dt = datetime.now(tz)
        try:
            sh = gc.open(config.REPORTS_SHEET_NAME)
        except: return

        try:
            worksheet = sh.worksheet(config.ERROR_LOG_WORKSHEET)
        except:
            worksheet = sh.add_worksheet(title=config.ERROR_LOG_WORKSHEET, rows=100, cols=3)
            worksheet.append_row(["–î–∞—Ç–∞ (–ï–ö–ë)", "–ò–º—è —Ñ–∞–π–ª–∞", "–¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏"])

        date_str = now_dt.strftime('%Y-%m-%d %H:%M:%S')
        worksheet.append_row([date_str, file_name, error_message])
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ –ª–æ–≥.")
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ª–æ–≥–∞: {e}")

def log_unknown_skus_batch(gc, unknown_skus_list, file_name):
    if not unknown_skus_list: return
    try:
        tz = pytz.timezone(TIMEZONE)
        now_dt = datetime.now(tz)
        date_str = now_dt.strftime('%Y-%m-%d %H:%M:%S')
        
        try: sh = gc.open(config.REPORTS_SHEET_NAME)
        except: return

        try: worksheet = sh.worksheet(config.ERROR_LOG_WORKSHEET)
        except:
            worksheet = sh.add_worksheet(title=config.ERROR_LOG_WORKSHEET, rows=100, cols=3)
            worksheet.append_row(["–î–∞—Ç–∞ (–ï–ö–ë)", "–ò–º—è —Ñ–∞–π–ª–∞", "–¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏"])
        
        rows = [[date_str, file_name, f"UNKNOWN_SKU: {sku}"] for sku in unknown_skus_list]
        worksheet.append_rows(rows)
        print(f"–ó–∞–ø–∏—Å–∞–Ω–æ {len(rows)} –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö SKU.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ SKU: {e}")

# --- 6. –°–ü–†–ê–í–û–ß–ù–ò–ö BRANDS ---

def normalize_text(text):
    if pd.isna(text): return ""
    return re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]', '', str(text).strip()).lower()

def load_brands_reference(client):
    print(f"–ó–∞–≥—Ä—É–∂–∞—é —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∏–∑ '{config.BRANDS_SHEET_NAME}'...")
    try: sh = client.open(config.BRANDS_SHEET_NAME)
    except Exception as e: raise RuntimeError(f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ç–∞–±–ª–∏—Ü—ã: {e}")

    ws = None
    # –õ–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ –ª–∏—Å—Ç–∞ (–∫–∞–∫ –±—ã–ª–∞)
    wn = getattr(config, "BRANDS_WORKSHEET_NAME", None)
    if wn:
        try: ws = sh.worksheet(wn)
        except: pass
    
    if ws is None:
        for w in sh.worksheets():
            if any(str(h).strip() == "brand_1c" for h in w.row_values(1)):
                ws = w; break
    
    if ws is None:
        for w in sh.worksheets():
            if "brand" in w.title.lower(): ws = w; break
            
    if ws is None: raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç Brands.")

    df = pd.DataFrame(ws.get_all_records())
    df['brand_1c'] = df['brand_1c'].astype(str).fillna('')
    df['search_key'] = df['brand_1c'].apply(normalize_text)

    if 'Min_Batch' not in df.columns: df['Min_Batch'] = 1
    df['Coeff'] = pd.to_numeric(df['Coeff'], errors='coerce').fillna(0.5)
    df['Min_Stock'] = pd.to_numeric(df['Min_Stock'], errors='coerce').fillna(0).astype(int)
    df['Min_Batch'] = pd.to_numeric(df['Min_Batch'], errors='coerce').fillna(1).astype(int)
    df['Shipping_Days'] = df['Shipping_Days'].astype(str).str.strip().replace({'nan': '', '': '–ü–Ω, –í—Ç, –°—Ä, –ß—Ç, –ü—Ç'})

    return df

# --- 7. –°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò –ü–ê–†–°–ò–ù–ì (–û–ë–ù–û–í–õ–ï–ù–û) ---

def find_latest_file(drive_service, folder_id):
    """–ò—â–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID + MimeType."""
    print(f"–ò—â—É —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ: {folder_id}")
    query = f"'{folder_id}' in parents and trashed=false"
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º mimeType, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —ç—Ç–æ Google –¢–∞–±–ª–∏—Ü–∞ –∏–ª–∏ XLSX
    response = drive_service.files().list(
        q=query, orderBy='createdTime desc', pageSize=1, fields='files(id, name, mimeType)'
    ).execute()
    files = response.get('files', [])
    if not files: return None
    print(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {files[0]['name']} (–¢–∏–ø: {files[0]['mimeType']})")
    return files[0]

def clean_number(value):
    if value is None or pd.isna(value) or value == '': return 0.0
    try:
        return float(value)
    except:
        s = str(value).replace('\xa0', '').replace(' ', '').replace(',', '.')
        s = re.sub(r'[^\d\.-]', '', s) # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, —Ç–æ—á–∫–∏ –∏ –º–∏–Ω—É—Å—ã
        try: return float(s)
        except: return 0.0

def download_file_content(drive_service, file_info):
    """–£–º–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: –µ—Å–ª–∏ Google Sheet -> –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, –∏–Ω–∞—á–µ -> –∫–∞—á–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å."""
    file_id = file_info['id']
    mime_type = file_info.get('mimeType', '')
    file_content = io.BytesIO()

    if mime_type == 'application/vnd.google-apps.spreadsheet':
        print("–≠—Ç–æ Google –¢–∞–±–ª–∏—Ü–∞. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –≤ Excel...")
        request = drive_service.files().export_media(
            fileId=file_id, 
            mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
    else:
        print("–°–∫–∞—á–∏–≤–∞—é –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª (XLSX/CSV)...")
        request = drive_service.files().get_media(fileId=file_id)

    downloader = MediaIoBaseDownload(file_content, request)
    done = False
    while done is False:
        status, done = downloader.next_chunk()
    
    file_content.seek(0)
    return file_content

def download_and_parse_report(drive_service, file_info, file_name):
    # 1. –°–∫–∞—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
    file_content = download_file_content(drive_service, file_info)
    
    df_raw = None
    success = False
    
    # 2. –ü—Ä–æ–±—É–µ–º Excel (XLSX/XLS)
    try:
        print("–ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è –∫–∞–∫ Excel...")
        df_raw = pd.read_excel(file_content, header=None, engine='openpyxl')
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ñ–∞–π–ª –æ—Ç–∫—Ä—ã–ª—Å—è, –µ—Å—Ç—å –ª–∏ –≤ –Ω–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü–∞?
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫ –≤ —Ç–µ–∫—Å—Ç
        check_str = df_raw.head(20).astype(str).to_string().lower()
        if "–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞" in check_str or "–æ—Å—Ç–∞—Ç–æ–∫" in check_str:
            success = True
            print("–£—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç –∫–∞–∫ Excel.")
    except Exception as e:
        print(f"–ù–µ Excel: {e}")
        file_content.seek(0)
    
    # 3. –ï—Å–ª–∏ Excel –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º CSV —Å –ø–µ—Ä–µ–±–æ—Ä–æ–º –∫–æ–¥–∏—Ä–æ–≤–æ–∫
    if not success:
        # 1C –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç cp1251, —Å—Ç–∞–≤–∏–º –µ–≥–æ –ø–µ—Ä–≤—ã–º
        encodings_to_try = ['cp1251', 'utf-8', 'utf-8-sig', 'latin1']
        
        for enc in encodings_to_try:
            print(f"–ü—Ä–æ–±—É—é –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–∫ CSV (–∫–æ–¥–∏—Ä–æ–≤–∫–∞ {enc})...")
            try:
                file_content.seek(0)
                temp_df = pd.read_csv(
                    file_content, 
                    header=None, 
                    encoding=enc, 
                    sep=None,     # –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è (; –∏–ª–∏ ,)
                    engine='python', 
                    on_bad_lines='skip'
                )
                
                # –ü–†–û–í–ï–†–ö–ê: –ï—Å–ª–∏ –º—ã —É–≥–∞–¥–∞–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∫—É, –º—ã –¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                check_str = temp_df.head(20).astype(str).to_string().lower()
                
                # –ò—â–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ, —á—Ç–æ–±—ã –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å, —á—Ç–æ —ç—Ç–æ –Ω–µ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã
                if "–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞" in check_str or "–æ—Å—Ç–∞—Ç–æ–∫" in check_str or "–ø—Ä–æ–¥–∞–∂–∏" in check_str:
                    df_raw = temp_df
                    print(f"--> –£—Å–ø–µ—Ö! –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞: {enc}")
                    success = True
                    break
                else:
                    print(f"--> –§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–≤–µ—Ä–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞).")
            except Exception as e:
                print(f"--> –û—à–∏–±–∫–∞ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {enc}: {e}")
                continue

    if df_raw is None or df_raw.empty:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞.")

    # 4. –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    header_row_idx = -1
    max_matches = 0
    keywords = ["–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞", "–æ—Å—Ç–∞—Ç–æ–∫", "—Ä–µ–∑–µ—Ä–≤", "–ø—Ä–æ–¥–∞–∂–∏", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "—Å–∫–ª–∞–¥"]
    
    # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ –ø–µ—Ä–≤—ã—Ö 50 —Å—Ç—Ä–æ–∫–∞—Ö
    for idx in range(min(50, len(df_raw))):
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫—É, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        row_str = " ".join(df_raw.iloc[idx].astype(str).fillna('').str.lower().tolist())
        
        # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞—à–ª–æ—Å—å –≤ —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–µ
        matches = sum(1 for kw in keywords if kw in row_str)
        
        if matches >= 2: # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã 2 —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞" –∏ "–û—Å—Ç–∞—Ç–æ–∫")
            if matches > max_matches:
                max_matches = matches
                header_row_idx = idx

    if header_row_idx == -1:
        # –î–ï–ë–ê–ì: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤—ã–≤–µ–¥–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –≤ –ª–æ–≥, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –≤–∏–¥–∏—Ç –±–æ—Ç
        print("!!! –ó–ê–ì–û–õ–û–í–û–ö –ù–ï –ù–ê–ô–î–ï–ù. –í–û–¢ –ß–¢–û –í–ò–î–ò–¢ –ë–û–¢ –í –ü–ï–†–í–´–• 10 –°–¢–†–û–ö–ê–•: !!!")
        print(df_raw.head(10).to_string())
        print("---------------------------------------------------------------------")
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã (—Å—Ç—Ä–æ–∫–∞ —Å '–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞', '–û—Å—Ç–∞—Ç–æ–∫'...).")

    print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–æ–∫–µ {header_row_idx + 1}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    df_raw.columns = df_raw.iloc[header_row_idx]
    df_raw = df_raw.iloc[header_row_idx + 1:].reset_index(drop=True)
    df_raw.columns = df_raw.columns.astype(str).str.strip().str.lower()
    
    col_names = df_raw.columns.tolist()
    
    def find_col(kws, required=True):
        for kw in kws:
            for c in col_names:
                if kw in c: return c
        if required: raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ {kws}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {col_names}")
        return None
        
    real_nom = find_col(["–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"])
    real_stock = find_col(["—É—á–∏—Ç—ã–≤–∞—è —Ä–µ–∑–µ—Ä–≤", "–æ—Å—Ç–∞—Ç–æ–∫", "—Å–∫–ª–∞–¥"])
    real_sales = find_col(["–ø—Ä–æ–¥–∞–∂–∏", "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"])
    real_pack = find_col(["–≤–ª–æ–∂–µ–Ω–∏–µ", "—É–ø–∞–∫–æ–≤–∫–∞"], required=False)
    
    rename_map = {real_nom: COL_NAME_NOMENCLATURE, real_stock: COL_NAME_STOCK, real_sales: COL_NAME_SALES}
    if real_pack: rename_map[real_pack] = COL_NAME_PACK

    df_report = df_raw.rename(columns=rename_map)
    df_report = df_report.loc[:, ~df_report.columns.duplicated()]
    
    if COL_NAME_PACK not in df_report.columns:
        df_report[COL_NAME_PACK] = 1
        
    df_report['pack_size'] = df_report[COL_NAME_PACK].apply(clean_number).replace(0, 1)
    df_report['stock_raw'] = df_report[COL_NAME_STOCK].apply(clean_number)
    df_report['sales_raw'] = df_report[COL_NAME_SALES].apply(clean_number)
    
    # --- –û–ß–ò–°–¢–ö–ê ---
    df_report = df_report[~((df_report['stock_raw'] == 0) & (df_report['sales_raw'] == 0))]
    df_report[COL_NAME_NOMENCLATURE] = df_report[COL_NAME_NOMENCLATURE].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()

    JUNK_PHRASES = [
        "–ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –≤–æ—Å—Ç–æ—á–Ω–æ–π –∫—É—Ö–Ω–∏", "—Å–æ—É—Å—ã –ø–æ—Ä—Ü–∏–æ–Ω–Ω—ã–µ", "–±—Ä–µ–Ω–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–æ—É—Å",
        "–±—Ä–µ–Ω–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–º–±–∏—Ä—å", "–¥–ª—è –æ—Ñ–∏—Å–∞", "–≤–Ω–µ–ø—Ä–∞–π—Å–æ–≤—ã–π –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç", "–∏—Ç–æ–≥–æ", "–∑–∞ –Ω–∞–ª–∏—á–∫—É"
    ]
    pattern = '|'.join(JUNK_PHRASES)
    df_report = df_report[~df_report[COL_NAME_NOMENCLATURE].str.lower().str.contains(pattern, na=False)]
    df_report = df_report[~df_report[COL_NAME_NOMENCLATURE].str.match(r'^\s*\d+(\.\d+)*\.?\s', na=False)]
    df_report = df_report[df_report[COL_NAME_NOMENCLATURE].str.len() > 3]

    df_report['stock_box'] = df_report['stock_raw'] / df_report['pack_size']
    df_report['sales_box'] = df_report['sales_raw'] / df_report['pack_size']
    
    return df_report[[COL_NAME_NOMENCLATURE, COL_NAME_STOCK, COL_NAME_SALES, 'stock_box', 'sales_box']]
    
    # --- –û–ß–ò–°–¢–ö–ê –ú–£–°–û–†–ê (–û–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–¥ –≤–∞—à —Ñ–∞–π–ª) ---
    
    # 1. –£–¥–∞–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
    df_report = df_report[~((df_report['stock_raw'] == 0) & (df_report['sales_raw'] == 0))]

    # 2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
    df_report[COL_NAME_NOMENCLATURE] = df_report[COL_NAME_NOMENCLATURE].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()

    # 3. –§–∏–ª—å—Ç—Ä —Ñ—Ä–∞–∑ (–¥–æ–±–∞–≤–ª–µ–Ω—ã —Ñ—Ä–∞–∑—ã –∏–∑ –≤–∞—à–µ–≥–æ CSV)
    JUNK_PHRASES = [
        "–ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –≤–æ—Å—Ç–æ—á–Ω–æ–π –∫—É—Ö–Ω–∏",
        "—Å–æ—É—Å—ã –ø–æ—Ä—Ü–∏–æ–Ω–Ω—ã–µ",
        "–±—Ä–µ–Ω–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–æ—É—Å",
        "–±—Ä–µ–Ω–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–º–±–∏—Ä—å",
        "–¥–ª—è –æ—Ñ–∏—Å–∞",
        "–≤–Ω–µ–ø—Ä–∞–π—Å–æ–≤—ã–π –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç",
        "–∏—Ç–æ–≥–æ",
        "–∑–∞ –Ω–∞–ª–∏—á–∫—É"
    ]
    pattern = '|'.join(JUNK_PHRASES)
    df_report = df_report[~df_report[COL_NAME_NOMENCLATURE].str.lower().str.contains(pattern, na=False)]

    # 4. –£–¥–∞–ª—è–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä "1. –ü—Ä–æ–¥—É–∫—Ç—ã", "23. –ë—Ä–µ–Ω–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π")
    # –†–µ–≥—É–ª—è—Ä–∫–∞ –ª–æ–≤–∏—Ç —Å—Ç—Ä–æ–∫–∏ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–∫–∏ (–¥–∞–∂–µ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥ –Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª)
    df_report = df_report[~df_report[COL_NAME_NOMENCLATURE].str.match(r'^\s*\d+(\.\d+)*\.?\s', na=False)]

    # 5. –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–º–µ–Ω—å—à–µ 3 –±—É–∫–≤)
    df_report = df_report[df_report[COL_NAME_NOMENCLATURE].str.len() > 3]

    # –†–∞—Å—á–µ—Ç –∫–æ—Ä–æ–±–æ–∫ (–µ—Å–ª–∏ –≤ —Ñ–∞–π–ª–µ —É–∂–µ –∫–æ—Ä–æ–±–∫–∏, pack_size=1, –µ—Å–ª–∏ —à—Ç—É–∫–∏ - –ø–æ–¥–µ–ª–∏–º)
    df_report['stock_box'] = df_report['stock_raw'] / df_report['pack_size']
    df_report['sales_box'] = df_report['sales_raw'] / df_report['pack_size']
    
    return df_report[[COL_NAME_NOMENCLATURE, COL_NAME_STOCK, COL_NAME_SALES, 'stock_box', 'sales_box']]

# --- 8. –†–ê–°–ß–ï–¢ ---

def get_planning_days(wd_idx):
    if wd_idx == 4: return ['–°–±', '–í—Å', '–ü–Ω'], ['–í—Ç', '–°—Ä']
    elif wd_idx in [5, 6]: return [], []
    else:
        d1 = (wd_idx + 1) % 7
        d2 = (wd_idx + 2) % 7
        d3 = (wd_idx + 3) % 7
        return [DAY_MAP[d1]], [DAY_MAP[d2], DAY_MAP[d3]]

def calc_row(row):
    sales = row['sales_box']
    coeff = row.get('Coeff')
    if pd.isna(coeff) or coeff == 0 or sales <= 0: return 0
        
    current = row['stock_box']
    min_stock = row['Min_Stock']
    min_batch = row['Min_Batch']
    
    target = max(sales * coeff, min_stock)
    need = target - current
    
    if need <= 0: return 0
    if current > min_stock and need < min_batch: return 0
        
    if min_batch > 1: return int(math.ceil(need / min_batch) * min_batch)
    else: return int(math.ceil(need))

def check_shipping(ship_days, targets):
    if not ship_days: return False
    avail = {d.strip() for d in ship_days.split(',')}
    for t in targets:
        if t in avail: return True
    return False

def calculate_logic(report_df, brands_df):
    tz = pytz.timezone(TIMEZONE)
    wd = datetime.now(tz).weekday()
    days_A, days_B = get_planning_days(wd)
    
    if not days_A:
        print("–í—ã—Ö–æ–¥–Ω–æ–π –¥–µ–Ω—å.")
        return pd.DataFrame(), pd.DataFrame(), [], [], []

    report_df['search_key'] = report_df[COL_NAME_NOMENCLATURE].apply(normalize_text)
    merged = pd.merge(report_df, brands_df, on='search_key', how='left')
    
    unknown_skus = merged[merged['Coeff'].isna()][COL_NAME_NOMENCLATURE].unique().tolist()
    merged['need_qty'] = merged.apply(calc_row, axis=1)

    base = merged[(merged['need_qty'] > 0)]
    crit = base['stock_box'] < base['Min_Stock']
    ship_A = base.apply(lambda r: check_shipping(r['Shipping_Days'], days_A), axis=1)
    
    plan_A = base[crit | ship_A].copy()
    plan_B = base[~(crit | ship_A) & base.apply(lambda r: check_shipping(r['Shipping_Days'], days_B), axis=1)].copy()
    
    return plan_A, plan_B, days_A, days_B, unknown_skus

# --- 9. TELEGRAM & SAVE ---

def format_plan_msg(df, days, is_forecast=False):
    if df.empty: return ""
    icon = "üîÆ" if is_forecast else "üóìÔ∏è"
    title = f"{icon} **–ü–õ–ê–ù –ù–ê {', '.join(days)} ({'–ü—Ä–æ–≥–Ω–æ–∑' if is_forecast else '–û–°–ù–û–í–ù–û–ô'})**\n\n"
    
    sort_cols = [c for c in ['–¢–∏–ø', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ë—Ä–µ–Ω–¥'] if c in df.columns]
    df = df.sort_values(by=sort_cols)

    body = ""
    total = 0
    for _, row in df.iterrows():
        alert = "üî•" if row['stock_box'] < row['Min_Stock'] else ""
        cat = row.get('–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '')
        ptype = row.get('–¢–∏–ø', '‚Äî')
        info = f"{cat}, {ptype}" if cat else ptype
        
        body += f"{alert}*_{row.get('–ë—Ä–µ–Ω–¥', row[COL_NAME_NOMENCLATURE])}_* ({info}) ‚Äî **{int(row['need_qty'])}** –∫–æ—Ä.\n"
        total += row['need_qty']
        
    return title + body + f"\n–í–°–ï–ì–û: **{int(total)}** –∫–æ—Ä.\n"

def send_telegram(bot, chat_id, text):
    if not text: return
    MAX = 4000
    parts, curr = [], ""
    for line in text.split('\n'):
        if len(curr) + len(line) + 1 <= MAX: curr += line + '\n'
        else:
            parts.append(curr); curr = line + '\n'
    if curr: parts.append(curr)
    
    for p in parts:
        try: bot.send_message(chat_id, p, parse_mode='Markdown')
        except: pass

def save_plan_to_sheet(gc, df, day_str):
    if df.empty: return
    sname = config.REPORT_WORKSHEET_PREFIX + day_str
    
    out = df.copy().rename(columns={'brand_1c':'1–° –ò–º—è', 'stock_box':'–û—Å—Ç–∞—Ç–æ–∫', 'need_qty':'–ü–õ–ê–ù'})
    cols = ['–¢–∏–ø', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ë—Ä–µ–Ω–¥', '1–° –ò–º—è', '–û—Å—Ç–∞—Ç–æ–∫', '–ü–õ–ê–ù']
    out = out[[c for c in cols if c in out.columns]]
    
    try: sh = gc.open(config.REPORTS_SHEET_NAME)
    except: return

    try: sh.del_worksheet(sh.worksheet(sname))
    except: pass
    
    ws = sh.add_worksheet(title=sname, rows=len(out)+5, cols=len(out.columns))
    ws.update([out.columns.tolist()] + out.values.tolist())

def archive_file(drive, file_id):
    try:
        drive.files().update(fileId=file_id, addParents=config.ARCHIVE_FOLDER_ID,
                             removeParents=config.INPUT_FOLDER_ID).execute()
        print("–§–∞–π–ª –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏: {e}")

# --- MAIN ---

def main():
    f_log = "System"
    gc = None
    bot = telebot.TeleBot(config.TELEGRAM_TOKEN)
    
    try:
        print("--- –ó–∞–ø—É—Å–∫ ---")
        gc, drive = connect_services()
        
        brands_df = load_brands_reference(gc)
        file_info = find_latest_file(drive, config.INPUT_FOLDER_ID)
        
        if not file_info:
            print("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤.")
            return
            
        f_log = file_info['name']
        # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å –æ–±—ä–µ–∫—Ç file_info, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ ID
        report_df = download_and_parse_report(drive, file_info, f_log)
        
        plan_A, plan_B, days_A, days_B, unknown_skus = calculate_logic(report_df, brands_df)
        
        if not days_A: return
        
        if unknown_skus:
            log_unknown_skus_batch(gc, unknown_skus, f_log)
            send_telegram(bot, config.TELEGRAM_CHAT_ID, f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ SKU ({len(unknown_skus)} —à—Ç). –°–º. –ª–æ–≥.")

        msg_A = format_plan_msg(plan_A, days_A)
        msg_B = format_plan_msg(plan_B, days_B, True)
        
        txt_A = msg_A if msg_A else f"*–ü–ª–∞–Ω –ê –Ω–∞ {', '.join(days_A)} –ø—É—Å—Ç.*"
        head = f"‚úÖ **–û–¢–ß–ï–¢ {datetime.now(pytz.timezone(TIMEZONE)).strftime('%d.%m.%Y')}**\n\n"
        
        send_telegram(bot, config.TELEGRAM_CHAT_ID, head + txt_A)
        if msg_B: send_telegram(bot, config.TELEGRAM_CHAT_ID, msg_B)
        
        save_plan_to_sheet(gc, plan_A, datetime.now(pytz.timezone(TIMEZONE)).strftime('%d.%m'))
        archive_file(drive, file_info['id'])
        print("--- –ì–æ—Ç–æ–≤–æ ---")

    except Exception as e:
        err = f"‚ùå –û—à–∏–±–∫–∞ '{f_log}':\n{e}"
        print(err)
        try: bot.send_message(config.TELEGRAM_CHAT_ID, err)
        except: pass
        if gc: log_error_to_sheet(gc, err, f_log)
        return

if __name__ == "__main__":
    main()